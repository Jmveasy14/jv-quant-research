# In file: .github/workflows/scheduler.yml

# Name of the workflow
name: Daily Data Pipeline

# --- Triggers ---
# This workflow runs on a schedule (cron)
on:
  schedule:
    # Runs every day at 05:00 UTC
    - cron: '0 5 * * *'
  # This line allows you to manually run the workflow from the Actions tab
  workflow_dispatch:

# --- Jobs ---
jobs:
  # The "run-pipeline" job
  run-pipeline:
    runs-on: ubuntu-latest

    # Give the job permissions to write back to the repository
    permissions:
      contents: write

    steps:
      # 1. Check out your repository code
      - name: Check out repository
        uses: actions/checkout@v3

      # 2. Set up Python
      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      # 4. Run the main data pipeline script
      # This will create/update the asset_universe.duckdb file
      - name: Run create_db.py to generate data
        run: |
          python create_db.py

      # 5. Commit the updated database file back to the repository
      - name: Commit updated database
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add asset_universe.duckdb
          # The following command will only commit if there are changes
          git commit -m "chore: Daily data pipeline run" || echo "No changes to commit"
          git push

